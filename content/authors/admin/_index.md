---
# Display name
name: Lindon Roberts

# Username (this should match the folder name)
authors:
- admin

# Is this the primary user of the site?
superuser: true

# Role/position
role: Lecturer

# Organizations/Affiliations
organizations:
- name: University of Sydney
  url: ""

# Short bio (displayed in user profile at end of posts)
bio: My research is in numerical analysis, particularly nonconvex and derivative-free optimization.

interests:
- Derivative-Free Optimization
- Nonconvex Optimization
- Numerical Analysis
- Data Science

education:
  courses:
  - course: DPhil in Mathematics
    institution: University of Oxford
    year: 2019
  - course: Bachelor of Computational Science (Honours)
    institution: Australian National University
    year: 2011

# Social/Academic Networking
# For available icons, see: https://sourcethemes.com/academic/docs/widgets/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "#contact" for contact widget.
social:
- icon: envelope
  icon_pack: fas
  link: 'mailto:lindon.roberts@sydney.edu.au'  # Was '#contact'. For a direct email link, use "mailto:test@example.org".
- icon: google-scholar
  icon_pack: ai
  link: https://scholar.google.co.uk/citations?user=s8Xj5BgAAAAJ
- icon: github
  icon_pack: fab
  link: https://github.com/lindonroberts
# Link to a PDF of your resume/CV from the About widget.
# To enable, copy your resume/CV to `static/files/cv.pdf` and uncomment the lines below.  
# - icon: cv
#   icon_pack: ai
#   link: files/cv.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: "lindon.roberts@sydney.edu.au"
  
# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.  
#user_groups:
#- Researchers
#- Visitors
---

I am a lecturer and ARC DECRA Fellow at the School of Mathematics and Statistics, University of Sydney. My research interests are in numerical analysis and data science, particularly nonconvex and derivative-free optimization. 

Details of my CV, publications, talks and software are below (or look at [Google Scholar](https://scholar.google.co.uk/citations?user=s8Xj5BgAAAAJ) and [Github](https://github.com/lindonroberts)). For general optimization resources, see below or my [nonlinear optimization resources](opt/) page.

**Recent news:**

- (Mar-23) New paper [*Model Construction for Convex-Constrained Derivative-Free Optimization*](https://arxiv.org/abs/2403.14960). This develops an approximation theory for quadratic interpolation in general convex-constrained sets, extending [earlier work on linear interpolation](https://doi.org/10.1137/21M1460971). 
- (Feb-23) Our paper [*Non-Uniform Smoothness for Gradient Descent*](https://openreview.net/pdf?id=17ESEjETbP) with [Albert Berahas](https://aberahas.engin.umich.edu/) (University of Michigan) and [Fred Roosta](https://people.smp.uq.edu.au/FredRoosta/) (University of Queensland) has been accepted by Transactions on Machine Learning Research.
- (Feb-23) I will be speaking about the paper [*Dynamic Bilevel Learning with Inexact Line Search*](https://arxiv.org/abs/2308.10098) as an invited speaker for the first [SigmaOpt workshop](https://www.mathematics.org.au/sys/pages/plain.php?page_id=25&conf_id=61).
- (Jan-23) Happy to receive a [CNRS International Emerging Actions](https://international.cnrs.fr/en/actualite/appel-iea-2023/) grant with [Clément Royer](https://www.lamsade.dauphine.fr/~croyer/) (Université Paris Dauphine-PSL) to work on random subspace methods for DFO.
- (Dec-23) I am a co-organiser of the joint [WOMBAT/WICO workshops](https://wombat.mocao.org/) on optimisation and computational maths, 11-15 December at the University of Sydney. I am also speaking about [expected decrease analysis for random subspace methods](https://arxiv.org/abs/2308.04734).
- (Nov-23) New paper [*Non-Uniform Smoothness for Gradient Descent*](https://arxiv.org/abs/2311.08615) with [Albert Berahas](https://aberahas.engin.umich.edu/) (University of Michigan) and [Fred Roosta](https://people.smp.uq.edu.au/FredRoosta/) (University of Queensland). We introduce a new local first-order smoothness oracle for automatic tuning of stepsizes for gradient descent.
- (Nov-23) Our paper [*Analyzing Inexact Hypergradients for Bilevel Learning*](https://doi.org/10.1093/imamat/hxad035) with [Matthias Ehrhardt](https://mehrhardt.github.io/) (University of Bath) has been accepted by IMA Journal of Applied Mathematics. 
- (Nov-23) I am on the judging panel for the first SigmaOpt Student Best Paper Prize (for Australian students working in optimization), which will be presented at the [SigmaOpt workshop](https://www.mathematics.org.au/sys/pages/plain.php?page_id=25&conf_id=61) after ANZIAM 2024. See the workshop page for details.
- (Oct-23) I will be an invited speaker at the first [SigmaOpt workshop](https://www.mathematics.org.au/sys/pages/plain.php?page_id=25&conf_id=61) associated with the ANZIAM meeting in February 2024. I am also on the judging panel for the best student paper prize which will be awarded at the workshop. 
- (Oct-23) I gave a One School seminar (i.e. school colloquium) for the USyd School of Mathematics and Statistics.
- (Sep-23) I will be speaking about large-scale DFO at the [Simons Collaboration on Hidden Symmetries and Fusion Energy](https://hiddensymmetries.princeton.edu/meetings/simons-hour-talks).
- (Aug-23) Very excited to receive an [ARC Discovery Early Career Researcher Award](https://www.arc.gov.au/funding-research/funding-schemes/discovery-program/discovery-early-career-researcher-award-decra) (2024-2026). See a short project description on the [ARC page of all funded projects](https://rms.arc.gov.au/RMS/Report/Download/Report/1b0c8b2e-7bb0-4f2d-8f52-ad207cfbb41d/252) or the [university news item](https://www.sydney.edu.au/news-opinion/news/2023/08/28/early-career-researchers-awarded-5-1m-funding.html).
- (Aug-23) New paper [*Dynamic Bilevel Learning with Inexact Line Search*](https://arxiv.org/abs/2308.10098) with [Mohammad Sadegh Salehi](https://mohammadsadeghsalehi.github.io/), [Matthias Ehrhardt](https://mehrhardt.github.io/) (University of Bath) and [Subhadip Mukherjee](https://sites.google.com/view/subhadip-mukherjee/home) (IIT Kharagpur). We introduce a linesearch algorithm suitable for bilevel learning based on conntrollable accuracy function and (hyper)gradient evaluations.
- (Aug-23) New paper [*Expected decrease for derivative-free algorithms using random subspaces*](https://arxiv.org/abs/2308.04734) with [Warren Hare](https://cmps.ok.ubc.ca/about/contact/warren-hare/) (University of British Columbia) and [Clément Royer](https://www.lamsade.dauphine.fr/~croyer/) (Université Paris Dauphine-PSL). We study why random subspace methods work best with very low-dimensional subspaces.

[News archive]({{< ref "news/archive.md" >}})

**Awards:**

- [ARC DECRA fellowship](https://www.arc.gov.au/funding-research/funding-schemes/discovery-program/discovery-early-career-researcher-award-decra) (2024-2026).
- [IMA Leslie Fox Prize for Numerical Analysis](https://ima.org.uk/awards-medals/ima-leslie-fox-prize-numerical-analysis/) (2021).
- Reddick Prize, Mathematical Institute, University of Oxford (2020).
- [Best paper award, *Mathematical Programming Computation*](https://www.springer.com/journal/12532/updates/17226372) (2019).