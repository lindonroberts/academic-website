---
# Display name
name: Lindon Roberts

# Username (this should match the folder name)
authors:
- admin

# Is this the primary user of the site?
superuser: true

# Role/position
role: Lecturer

# Organizations/Affiliations
organizations:
- name: University of Sydney
  url: ""

# Short bio (displayed in user profile at end of posts)
bio: My research is in numerical analysis, particularly nonconvex and derivative-free optimization.

interests:
- Derivative-Free Optimization
- Nonconvex Optimization
- Numerical Analysis
- Data Science

education:
  courses:
  - course: DPhil in Mathematics
    institution: University of Oxford
    year: 2019
  - course: Bachelor of Computational Science (Honours)
    institution: Australian National University
    year: 2011

# Social/Academic Networking
# For available icons, see: https://sourcethemes.com/academic/docs/widgets/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "#contact" for contact widget.
social:
- icon: envelope
  icon_pack: fas
  link: 'mailto:lindon.roberts@sydney.edu.au'  # Was '#contact'. For a direct email link, use "mailto:test@example.org".
- icon: google-scholar
  icon_pack: ai
  link: https://scholar.google.co.uk/citations?user=s8Xj5BgAAAAJ
- icon: github
  icon_pack: fab
  link: https://github.com/lindonroberts
# Link to a PDF of your resume/CV from the About widget.
# To enable, copy your resume/CV to `static/files/cv.pdf` and uncomment the lines below.  
# - icon: cv
#   icon_pack: ai
#   link: files/cv.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: "lindon.roberts@sydney.edu.au"
  
# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.  
#user_groups:
#- Researchers
#- Visitors
---

I am a lecturer and ARC DECRA Fellow at the School of Mathematics and Statistics, University of Sydney. My research interests are in numerical analysis and data science, particularly nonconvex and derivative-free optimization. 

Details of my CV, publications, talks and software are below (or look at [Google Scholar](https://scholar.google.co.uk/citations?user=s8Xj5BgAAAAJ) and [Github](https://github.com/lindonroberts)). For general optimization resources, see below or my [nonlinear optimization resources](opt/) page.

**Recent news:**

- (Aug-24) Our paper [*Expected decrease for derivative-free algorithms using random subspaces*](https://doi.org/10.1090/mcom/4011) with [Warren Hare](https://cmps.ok.ubc.ca/about/contact/warren-hare/) (University of British Columbia) and [Clément Royer](https://www.lamsade.dauphine.fr/~croyer/) (Université Paris Dauphine-PSL) has been accepted by Mathematics of Computation.
- (Jul-24) New paper [*Black-box Optimization Algorithms for Regularized Least-squares Problems*](https://arxiv.org/abs/2407.14915) with [Yanjun Liu](https://yanjunliu-regina.github.io/) (Princeton University) and Kevin Lam (Australian National University). We introduce a DFO method for nonlinear least-squares problems with nonsmooth regularizers.
- (Jun-24) I will be visiting [Clément Royer](https://www.lamsade.dauphine.fr/~croyer/) at the Université Paris Dauphine-PSL and then attending the [2nd Derivative-Free Optimization Symposium](https://sites.google.com/diag.uniroma1.it/dfos24/home) at the University of Padova.
- (May-24) I will be hosting [Hung Phan](https://faculty.uml.edu/hung_phan/) (University of Massachusetts Lowell) as part of [SMRI's](https://mathematical-research-institute.sydney.edu.au/) International Visitor Program.
- (Apr-24) Significant new revisions to an earlier preprint, now titled [*An adaptively inexact first-order method for bilevel optimization with application to hyperparameter learning*](https://arxiv.org/abs/2308.10098), are now available. This is joint work with [Mohammad Sadegh Salehi](https://mohammadsadeghsalehi.github.io/), [Matthias Ehrhardt](https://mehrhardt.github.io/) (University of Bath) and [Subhadip Mukherjee](https://sites.google.com/view/subhadip-mukherjee/home) (IIT Kharagpur).
- (Mar-24) New paper [*Model Construction for Convex-Constrained Derivative-Free Optimization*](https://arxiv.org/abs/2403.14960). This develops an approximation theory for quadratic interpolation in general convex-constrained sets, extending [earlier work on linear interpolation](https://doi.org/10.1137/21M1460971). 
- (Feb-24) Hosted [Shane Henderson](https://people.orie.cornell.edu/shane/) (Cornell University) as part of [SMRI's](https://mathematical-research-institute.sydney.edu.au/) International Visitor Program.
- (Feb-24) Our paper [*Non-Uniform Smoothness for Gradient Descent*](https://openreview.net/pdf?id=17ESEjETbP) with [Albert Berahas](https://aberahas.engin.umich.edu/) (University of Michigan) and [Fred Roosta](https://people.smp.uq.edu.au/FredRoosta/) (University of Queensland) has been accepted by Transactions on Machine Learning Research.
- (Feb-24) I will be speaking about the paper [*Dynamic Bilevel Learning with Inexact Line Search*](https://arxiv.org/abs/2308.10098) as an invited speaker for the first [SigmaOpt workshop](https://www.mathematics.org.au/sys/pages/plain.php?page_id=25&conf_id=61).
- (Jan-24) Happy to receive a [CNRS International Emerging Actions](https://international.cnrs.fr/en/actualite/appel-iea-2023/) grant with [Clément Royer](https://www.lamsade.dauphine.fr/~croyer/) (Université Paris Dauphine-PSL) to work on random subspace methods for DFO.
- (Dec-23) I am a co-organiser of the joint [WOMBAT/WICO workshops](https://wombat.mocao.org/) on optimisation and computational maths, 11-15 December at the University of Sydney. I am also speaking about [expected decrease analysis for random subspace methods](https://arxiv.org/abs/2308.04734).
- (Nov-23) New paper [*Non-Uniform Smoothness for Gradient Descent*](https://arxiv.org/abs/2311.08615) with [Albert Berahas](https://aberahas.engin.umich.edu/) (University of Michigan) and [Fred Roosta](https://people.smp.uq.edu.au/FredRoosta/) (University of Queensland). We introduce a new local first-order smoothness oracle for automatic tuning of stepsizes for gradient descent.
- (Nov-23) Our paper [*Analyzing Inexact Hypergradients for Bilevel Learning*](https://doi.org/10.1093/imamat/hxad035) with [Matthias Ehrhardt](https://mehrhardt.github.io/) (University of Bath) has been accepted by IMA Journal of Applied Mathematics. 
- (Nov-23) I am on the judging panel for the first SigmaOpt Student Best Paper Prize (for Australian students working in optimization), which will be presented at the [SigmaOpt workshop](https://www.mathematics.org.au/sys/pages/plain.php?page_id=25&conf_id=61) after ANZIAM 2024. See the workshop page for details.

[News archive]({{< ref "news/archive.md" >}})

**Awards:**

- [ARC DECRA fellowship](https://www.arc.gov.au/funding-research/funding-schemes/discovery-program/discovery-early-career-researcher-award-decra) (2024-2026).
- [IMA Leslie Fox Prize for Numerical Analysis](https://ima.org.uk/awards-medals/ima-leslie-fox-prize-numerical-analysis/) (2021).
- Reddick Prize, Mathematical Institute, University of Oxford (2020).
- [Best paper award, *Mathematical Programming Computation*](https://www.springer.com/journal/12532/updates/17226372) (2019).